{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOB6b44o81yIU+o5dI+BX00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taekyungss/computer_vision_planting_grass/blob/main/VIT_pytorch_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아직 미완"
      ],
      "metadata": {
        "id": "xrszysPBUdFB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubeK2iSeLs2C"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from torch import optim\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from torchvision import utils\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "BKuVQDIwMD6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path2data = '/content/data'\n",
        "\n",
        "if not os.path.exists(path2data):\n",
        "  os.mkdir(path2data)\n",
        "\n",
        "train_ds = datasets.STL10(path2data, split=\"train\", download=True, transform=transforms.ToTensor())\n",
        "val_ds = datasets.STL10(path2data, split=\"test\", download=True, transform = transforms.ToTensor())\n",
        "\n",
        "print(len(train_ds))\n",
        "print(len(val_ds))"
      ],
      "metadata": {
        "id": "3XYwY52lMmWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformation\n",
        "\n",
        "transformation = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224)\n",
        "])\n",
        "\n",
        "train_ds.transform = transformation\n",
        "val_ds.transform = transformation\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle = True)\n",
        "val_dl = DataLoader(val_ds, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "oRbBz3SvNX2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show(img, y=None):\n",
        "  npimg = img.numpy()\n",
        "  npimg_tr = np.transpose(npimg, (1,2,0))\n",
        "  plt.imshow(npimg_tr)\n",
        "\n",
        "  if y is not None:\n",
        "    plt.title(\"labels: \"+str(y))\n",
        "\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "grid_size=4\n",
        "rnd_ind = np.random.randint(0,len(train_ds), grid_size)\n",
        "\n",
        "x_grid = [train_ds[i][0] for i in rnd_ind]\n",
        "y_grid = [val_ds[i][1] for i in rnd_ind]\n",
        "\n",
        "x_grid = utils.make_grid(x_grid, nrow=grid_size, padding=2)\n",
        "plt.figure(figsize=(10,10))\n",
        "show(x_grid, y_grid)"
      ],
      "metadata": {
        "id": "1Co2N7geQcUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VIT 구현"
      ],
      "metadata": {
        "id": "iHV0pgsNQ_Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# patch embedding\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, in_channels=3, patch_size = 16, emb_size = 768, img_size = 224):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.projection = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, emb_size, patch_size , stride=patch_size),\n",
        "        # einops.rearrange는multidimensional tensor를 쉽게 reordering하는 함수입니다.\n",
        "        Rearrange('b e (h) (w) -> b (h w) e')\n",
        "    )\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
        "    self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 +1, emb_size))\n",
        "\n",
        "  def forward(self, x:Tensor) -> Tensor:\n",
        "    b = x.shape[0]\n",
        "    x = self.projection(x)\n",
        "    cls_tokens = repeat( self.cls_token, '() n e -> b n e', b=b)\n",
        "    # elnops -> repeat\n",
        "    x = torch.cat([cls_tokens, x], dim=1)\n",
        "    x+= self.positions\n",
        "    return x"
      ],
      "metadata": {
        "id": "sPQ3XdN-Q8r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "\n",
        "x = torch.randn(16,3,224,224).to(device)\n",
        "patch_embedding = PatchEmbedding().to(device)\n",
        "patch_output = patch_embedding(x)\n",
        "print(\"[batch, 1+num of patches, emb_size] = \", patch_output.shape)"
      ],
      "metadata": {
        "id": "3cYV5bK4SuR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multihead attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,emb_size = 768, num_heads=8, dropout = 0):\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.num_heads = num_heads\n",
        "    self.keys = nn.Linear(emb_size, emb_size)\n",
        "    self.queries = nn.Linear(emb_size, emb_size)\n",
        "    self.values = nn.Linear(emb_size, emb_size)\n",
        "    self.att_drop = nn.Dropout(dropout)\n",
        "    self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    queries = rearrange(self.queries(x), 'b n (h d) -> b h n d',\n",
        "                        h = self.num_heads)\n",
        "    keys = rearrange(self.keys(x), 'b n (h d) -> b h n d', h=self.num_heads)\n",
        "    values = rearrange(self.values(x),'b n (h d) -> b h n d',h = self.num_heads)\n",
        "    energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "\n",
        "    if mask is not None:\n",
        "      fill_value = torch.finfo(torch.float32).min\n",
        "      energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "    scaling = self.emb_size ** (1/2)\n",
        "    att = F.softmax(energy, dim =-1) / scaling\n",
        "    att = self.att_drop(att)\n",
        "    out = torch.einsum('bhal, bhiv -> bhav', att, values)\n",
        "    out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "    out = self.projection(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DwzbnOuWUKxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 잘 구현되었는지 확인\n",
        "\n",
        "MHA = MultiHeadAttention().to(device)\n",
        "MHA_output = MHA(patch_output)\n",
        "print(MHA_output.shape)"
      ],
      "metadata": {
        "id": "tJZauOvvbduY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual block\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "  def __init__(self, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    res = x\n",
        "    x = self.fn(x, **kwargs)\n",
        "    x += res\n",
        "    return x"
      ],
      "metadata": {
        "id": "kWbjDhyqbnSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Sequential):\n",
        "  def __init__(self, emb_size, expansion = 4, drop_p = 0):\n",
        "    super().__init__(\n",
        "        nn.Linear(emb_size, expansion * emb_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(drop_p),\n",
        "        nn.Linear(expansion * emb_size, emb_size)\n",
        "    )"
      ],
      "metadata": {
        "id": "arrPxtZpb6a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "x = torch.randn(16,1,128).to(device)\n",
        "model = FeedForwardBlock(128).to(device)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fx2ZLtfLcM45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformerEncoderBlock\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "  def __init__(self, emb_size = 768, drop_p=0., forward_expansion = 4, forward_drop_p = 0., **kwargs):\n",
        "    super().__init__(\n",
        "        ResidualAdd(nn.Sequential(\n",
        "            nn.LayerNorm(emb_size),\n",
        "            MultiHeadAttention(emb_size, **kwargs),\n",
        "            nn.Dropout(drop_p)\n",
        "        )),\n",
        "        ResidualAdd(nn.Sequential(\n",
        "          nn.LayerNorm(emb_size),\n",
        "          FeedForwardBlock(emb_size, expansion= forward_expansion,\n",
        "                         drop_p = forward_drop_p),\n",
        "        nn.Dropout(drop_p)\n",
        "    ))\n",
        "    )"
      ],
      "metadata": {
        "id": "V4LxvLqLccjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "\n",
        "model = TransformerEncoderBlock().to(device)\n",
        "output = model(patch_output).to(device)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "Nmjj87esdZ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Sequential):\n",
        "  def __init__(self, depth =12, **kwargs):\n",
        "    super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n"
      ],
      "metadata": {
        "id": "GtzQ2qJEdhYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerEncoderBlock().to(device)\n",
        "output = model(patch_output)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "9kR1kAjYeXap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classficiationHead\n",
        "\n",
        "class classificationHead(nn.Sequential):\n",
        "  def __init__(self, emb_size = 768, n_classes = 10):\n",
        "    super().__init__(\n",
        "        Reduce('b n e -> b e', reduction = \"mean\"),\n",
        "        nn.LayerNorm(emb_size),\n",
        "        nn.Linear(emb_size, n_classes)\n",
        "    )"
      ],
      "metadata": {
        "id": "R1S3jc1iee0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "x = torch.rand(16,1,768).to(device)\n",
        "model = classificationHead().to(device)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "u9L92fhZewY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--## VIT 코딩 -->"
      ],
      "metadata": {
        "id": "do-sDbQ4fOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dEDKj0yzfQFO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}